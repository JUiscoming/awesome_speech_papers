# About this repository


# End-to-End Speech Recognition (음성 인식) 
|year|conference|research organization|title|model|link|
|--|--|--|------|---|--|
|2006|ICML|Toronto University|Connectionist Temporal Classification: Labelling Unsegmented Sequence Data with Recurrent Neural Networks|CTC|[link](https://www.cs.toronto.edu/~graves/icml_2006.pdf)|
|2008|||Supervised Sequence Labelling with Recurrent Neural Networks|||
|2014|ICML||Towards End-to-End Speech Recognition with Recurrent Neural Networks|||
|2014|||Deep speech: Scaling up end-to-end speech recognition|||
|2015|NIPS||Attention-Based Models for Speech Recognition|Seq2Seq||
|2015|ICASSP|Google|Listen, Attend and Spell|Seq2Seq|[arxiv link](https://arxiv.org/pdf/1508.01211)|
|2016|ICML||Deep Speech 2 : End-to-End Speech Recognition in English and Mandarin|CTC-based CNN model||
|2016|||End-to-End Attention-based Large Vocabulary Speech Recognition|||
|2017|||Monotonic Chunkwise Attention|||
|2018|||Speech-Transformer: A No-Recurrence Sequence-to-Sequence Model for Speech Recognition|||
|2019|||Listen, Attend, Spell and Adapt: Speaker Adapted Sequence-to-Sequence ASR|||
|2019|Interspeech|Nvidia|Jasper: An End-to-End Convolutional Neural Acoustic Model|CTC-based CNN model||




<br>
 
# End-to-End Speech Synthesis (음성 합성)

<br>

# End-to-End Spoken Language Understanding (음성 언어 이해)

<br>

# Self-Supervised(or Semi-Supervised) Learning for Speech 
|year|conference|research organization|title|link|
|--|--|--|------|--|
|2019|-|Facebook AI Research (FAIR)|wav2vec: Unsupervised Pre-training for Speech Recognition|[arxiv link](https://arxiv.org/pdf/1904.05862)|
|2019|ICLR|Facebook AI Research (FAIR)|vq-wav2vec: Self-Supervised Learning of Discrete Speech Representations|[arxiv link](https://arxiv.org/pdf/1910.05453)|
|2020|-|Facebook AI Research (FAIR)|wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations|[arxiv link](https://arxiv.org/pdf/2006.11477)|
|2019|-|Deepmind|Learning robust and multilingual speech representations|[arxiv link](https://arxiv.org/pdf/2001.11128)|
|-|-|Deepmind|Self-Supervised Representations Improve End-to-End Speech Translation|[arxiv link](https://arxiv.org/pdf/1508.01211)|
|-|-|Deepmind|Unsupervised Pretraining Transfers Well Across Languages|[arxiv link](https://arxiv.org/pdf/1508.01211)|
|-|-|Deepmind|Learning Problem-agnostic Speech Representations from Multiple Self-supervised Tasks|[arxiv link](https://arxiv.org/pdf/1508.01211)|
|-|-|Deepmind|Learning robust and multilingual speech representations|[arxiv link](https://arxiv.org/pdf/1508.01211)|
|-|-|Deepmind|Problem-Agnostic Speech Embeddings for Multi-Speaker Text-to-Speech with SampleRNN|[arxiv link](https://arxiv.org/pdf/1508.01211)|
|2020|-|MIT CSAIL|SEMI-SUPERVISED SPEECH-LANGUAGE JOINT PRE- TRAINING FOR SPOKEN LANGUAGE UNDERSTANDING|[arxiv link](https://arxiv.org/pdf/2010.02295)|
